[06:43:30.486] Namespace(root_path='/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI', exp='SDCL', model='VNet', pre_max_iteration=2000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.001, deterministic=1, labelnum=8, gpu='0', seed=1345, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[06:43:31.705] train_lab set: total 8 samples
[06:43:31.705] total ['/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/06SR5RBREL16DQ6M8LWS/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/0RZDK210BSMWAA6467LU/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1D7CUD1955YZPGK8XHJX/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1GU15S0GJ6PFNARO469W/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1MHBF3G6DCPWHSKG7XCP/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/23X6SY44VT9KFHR7S7OC/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/2XL5HSFSE93RMOJDRGR4/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/38CWS74285MFGZZXR09Z/mri_norm2.h5'] samples
[06:43:31.709] train_lab set: total 8 samples
[06:43:31.710] total ['/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/06SR5RBREL16DQ6M8LWS/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/0RZDK210BSMWAA6467LU/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1D7CUD1955YZPGK8XHJX/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1GU15S0GJ6PFNARO469W/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1MHBF3G6DCPWHSKG7XCP/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/23X6SY44VT9KFHR7S7OC/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/2XL5HSFSE93RMOJDRGR4/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/38CWS74285MFGZZXR09Z/mri_norm2.h5'] samples
[06:43:31.712] 40 iterations per epoch
[06:43:31.713] 

[06:43:48.353] iteration 1 : loss: 0.809023, loss_dice: 0.619381, loss_ce: 0.998665
[06:43:55.246] iteration 2 : loss: 0.776671, loss_dice: 0.597845, loss_ce: 0.955496
[06:43:56.741] iteration 3 : loss: 0.785931, loss_dice: 0.565157, loss_ce: 1.006706
[06:43:58.223] iteration 4 : loss: 0.665670, loss_dice: 0.514858, loss_ce: 0.816481
[06:43:59.722] iteration 5 : loss: 0.720878, loss_dice: 0.552512, loss_ce: 0.889245
[06:44:01.201] iteration 6 : loss: 0.626831, loss_dice: 0.535774, loss_ce: 0.717888
[06:44:02.716] iteration 7 : loss: 0.655470, loss_dice: 0.538879, loss_ce: 0.772060
[06:44:04.337] iteration 8 : loss: 0.700820, loss_dice: 0.576487, loss_ce: 0.825153
[06:44:05.923] iteration 9 : loss: 0.647019, loss_dice: 0.522661, loss_ce: 0.771378
[06:44:07.431] iteration 10 : loss: 0.615736, loss_dice: 0.522275, loss_ce: 0.709197
[06:44:08.947] iteration 11 : loss: 0.630263, loss_dice: 0.535606, loss_ce: 0.724919
[06:44:10.452] iteration 12 : loss: 0.670949, loss_dice: 0.563697, loss_ce: 0.778202
[06:44:11.963] iteration 13 : loss: 0.626526, loss_dice: 0.531732, loss_ce: 0.721320
[06:44:13.481] iteration 14 : loss: 0.584581, loss_dice: 0.524292, loss_ce: 0.644871
[06:44:14.981] iteration 15 : loss: 0.638324, loss_dice: 0.555519, loss_ce: 0.721130
[06:44:16.500] iteration 16 : loss: 0.634524, loss_dice: 0.556194, loss_ce: 0.712854
[06:44:18.113] iteration 17 : loss: 0.584055, loss_dice: 0.532808, loss_ce: 0.635303
[06:44:19.715] iteration 18 : loss: 0.666695, loss_dice: 0.567086, loss_ce: 0.766303
[06:44:21.312] iteration 19 : loss: 0.521363, loss_dice: 0.488654, loss_ce: 0.554071
[06:44:22.934] iteration 20 : loss: 0.535987, loss_dice: 0.489677, loss_ce: 0.582298
[06:44:24.492] iteration 21 : loss: 0.536015, loss_dice: 0.497616, loss_ce: 0.574414
[06:44:26.008] iteration 22 : loss: 0.583518, loss_dice: 0.525945, loss_ce: 0.641091
[06:44:27.534] iteration 23 : loss: 0.545214, loss_dice: 0.510769, loss_ce: 0.579659
[06:44:29.046] iteration 24 : loss: 0.544053, loss_dice: 0.498306, loss_ce: 0.589799
[06:44:30.668] iteration 25 : loss: 0.570813, loss_dice: 0.533525, loss_ce: 0.608101
[06:44:32.286] iteration 26 : loss: 0.537754, loss_dice: 0.500269, loss_ce: 0.575240
[06:44:33.806] iteration 27 : loss: 0.498025, loss_dice: 0.464363, loss_ce: 0.531688
[06:44:35.325] iteration 28 : loss: 0.592705, loss_dice: 0.538146, loss_ce: 0.647265
[06:44:36.844] iteration 29 : loss: 0.587886, loss_dice: 0.522553, loss_ce: 0.653219
[06:44:38.377] iteration 30 : loss: 0.520495, loss_dice: 0.500252, loss_ce: 0.540738
[06:44:39.902] iteration 31 : loss: 0.603025, loss_dice: 0.538469, loss_ce: 0.667581
[06:44:41.421] iteration 32 : loss: 0.530934, loss_dice: 0.509333, loss_ce: 0.552535
[06:44:42.992] iteration 33 : loss: 0.511278, loss_dice: 0.490907, loss_ce: 0.531649
[06:44:44.637] iteration 34 : loss: 0.507734, loss_dice: 0.491652, loss_ce: 0.523815
[06:44:46.227] iteration 35 : loss: 0.536078, loss_dice: 0.499046, loss_ce: 0.573110
[06:44:47.748] iteration 36 : loss: 0.516046, loss_dice: 0.470749, loss_ce: 0.561342
[06:44:49.286] iteration 37 : loss: 0.451570, loss_dice: 0.438101, loss_ce: 0.465038
[06:44:50.846] iteration 38 : loss: 0.538252, loss_dice: 0.488556, loss_ce: 0.587948
[06:44:52.384] iteration 39 : loss: 0.444494, loss_dice: 0.438989, loss_ce: 0.449998
[06:44:53.913] iteration 40 : loss: 0.502738, loss_dice: 0.463060, loss_ce: 0.542416
[06:44:53.919] 

[06:44:55.480] iteration 41 : loss: 0.369668, loss_dice: 0.360502, loss_ce: 0.378835
[06:44:57.118] iteration 42 : loss: 0.525261, loss_dice: 0.491476, loss_ce: 0.559047
[06:44:58.753] iteration 43 : loss: 0.460210, loss_dice: 0.443789, loss_ce: 0.476630
[06:45:00.299] iteration 44 : loss: 0.481459, loss_dice: 0.449945, loss_ce: 0.512974
[06:45:01.860] iteration 45 : loss: 0.579384, loss_dice: 0.519912, loss_ce: 0.638856
[06:45:03.412] iteration 46 : loss: 0.500483, loss_dice: 0.453964, loss_ce: 0.547001
[06:45:04.982] iteration 47 : loss: 0.508519, loss_dice: 0.486690, loss_ce: 0.530348
[06:45:06.532] iteration 48 : loss: 0.451762, loss_dice: 0.425508, loss_ce: 0.478016
[06:45:08.089] iteration 49 : loss: 0.501925, loss_dice: 0.461574, loss_ce: 0.542276
[06:45:09.733] iteration 50 : loss: 0.504351, loss_dice: 0.466321, loss_ce: 0.542380
[06:45:11.369] iteration 51 : loss: 0.535703, loss_dice: 0.487452, loss_ce: 0.583955
[06:45:12.919] iteration 52 : loss: 0.467291, loss_dice: 0.439529, loss_ce: 0.495053
[06:45:14.478] iteration 53 : loss: 0.390303, loss_dice: 0.384152, loss_ce: 0.396454
[06:45:16.036] iteration 54 : loss: 0.474041, loss_dice: 0.459052, loss_ce: 0.489031
[06:45:17.600] iteration 55 : loss: 0.433785, loss_dice: 0.427717, loss_ce: 0.439852
[06:45:19.153] iteration 56 : loss: 0.500658, loss_dice: 0.470226, loss_ce: 0.531089
[06:45:20.709] iteration 57 : loss: 0.540288, loss_dice: 0.488922, loss_ce: 0.591655
[06:45:22.288] iteration 58 : loss: 0.383081, loss_dice: 0.373316, loss_ce: 0.392845
[06:45:23.970] iteration 59 : loss: 0.573497, loss_dice: 0.511524, loss_ce: 0.635469
[06:45:25.602] iteration 60 : loss: 0.507754, loss_dice: 0.467392, loss_ce: 0.548115
[06:45:27.177] iteration 61 : loss: 0.430911, loss_dice: 0.407286, loss_ce: 0.454535
[06:45:28.761] iteration 62 : loss: 0.421285, loss_dice: 0.412535, loss_ce: 0.430035
[06:45:30.338] iteration 63 : loss: 0.396000, loss_dice: 0.402392, loss_ce: 0.389609
[06:45:31.935] iteration 64 : loss: 0.408336, loss_dice: 0.392893, loss_ce: 0.423779
[06:45:33.515] iteration 65 : loss: 0.447444, loss_dice: 0.439368, loss_ce: 0.455520
[06:45:35.124] iteration 66 : loss: 0.433442, loss_dice: 0.425367, loss_ce: 0.441517
[06:45:36.810] iteration 67 : loss: 0.521803, loss_dice: 0.478461, loss_ce: 0.565144
[06:45:38.499] iteration 68 : loss: 0.521286, loss_dice: 0.468890, loss_ce: 0.573682
[06:45:40.113] iteration 69 : loss: 0.517220, loss_dice: 0.481008, loss_ce: 0.553433
[06:45:41.696] iteration 70 : loss: 0.498435, loss_dice: 0.491558, loss_ce: 0.505311
[06:45:43.287] iteration 71 : loss: 0.505150, loss_dice: 0.440431, loss_ce: 0.569870
[06:45:44.871] iteration 72 : loss: 0.403098, loss_dice: 0.389200, loss_ce: 0.416996
[06:45:46.470] iteration 73 : loss: 0.325600, loss_dice: 0.322374, loss_ce: 0.328825
[06:45:48.046] iteration 74 : loss: 0.371274, loss_dice: 0.375825, loss_ce: 0.366723
[06:45:49.744] iteration 75 : loss: 0.398667, loss_dice: 0.400769, loss_ce: 0.396564
[06:45:51.434] iteration 76 : loss: 0.395835, loss_dice: 0.385384, loss_ce: 0.406286
[06:45:53.008] iteration 77 : loss: 0.383837, loss_dice: 0.387481, loss_ce: 0.380193
[06:45:54.589] iteration 78 : loss: 0.457006, loss_dice: 0.447059, loss_ce: 0.466953
[06:45:56.164] iteration 79 : loss: 0.412130, loss_dice: 0.411393, loss_ce: 0.412866
[06:45:57.749] iteration 80 : loss: 0.533593, loss_dice: 0.471139, loss_ce: 0.596047
[06:45:57.750] 

[06:45:59.314] iteration 81 : loss: 0.394834, loss_dice: 0.391109, loss_ce: 0.398559
[06:46:00.876] iteration 82 : loss: 0.445826, loss_dice: 0.427247, loss_ce: 0.464405
[06:46:02.523] iteration 83 : loss: 0.575721, loss_dice: 0.495181, loss_ce: 0.656261
[06:46:04.193] iteration 84 : loss: 0.385936, loss_dice: 0.378498, loss_ce: 0.393374
[06:46:05.764] iteration 85 : loss: 0.349445, loss_dice: 0.337033, loss_ce: 0.361857
[06:46:07.325] iteration 86 : loss: 0.413406, loss_dice: 0.424441, loss_ce: 0.402371
[06:46:08.884] iteration 87 : loss: 0.410751, loss_dice: 0.411522, loss_ce: 0.409980
[06:46:10.440] iteration 88 : loss: 0.429369, loss_dice: 0.425007, loss_ce: 0.433731
[06:46:12.010] iteration 89 : loss: 0.412751, loss_dice: 0.384578, loss_ce: 0.440923
[06:46:13.575] iteration 90 : loss: 0.406628, loss_dice: 0.388043, loss_ce: 0.425212
[06:46:15.251] iteration 91 : loss: 0.428531, loss_dice: 0.415259, loss_ce: 0.441803
[06:46:16.888] iteration 92 : loss: 0.381459, loss_dice: 0.386831, loss_ce: 0.376087
[06:46:18.466] iteration 93 : loss: 0.504758, loss_dice: 0.467501, loss_ce: 0.542015
[06:46:20.042] iteration 94 : loss: 0.349972, loss_dice: 0.336536, loss_ce: 0.363408
[06:46:21.619] iteration 95 : loss: 0.427669, loss_dice: 0.412029, loss_ce: 0.443310
[06:46:23.176] iteration 96 : loss: 0.457980, loss_dice: 0.426202, loss_ce: 0.489758
[06:46:24.752] iteration 97 : loss: 0.292081, loss_dice: 0.304425, loss_ce: 0.279737
[06:46:26.329] iteration 98 : loss: 0.379233, loss_dice: 0.391223, loss_ce: 0.367242
[06:46:27.886] iteration 99 : loss: 0.436070, loss_dice: 0.405367, loss_ce: 0.466772
[06:46:29.523] iteration 100 : loss: 0.378527, loss_dice: 0.368376, loss_ce: 0.388678
[06:46:31.127] iteration 101 : loss: 0.367766, loss_dice: 0.352724, loss_ce: 0.382809
[06:46:32.678] iteration 102 : loss: 0.429485, loss_dice: 0.410633, loss_ce: 0.448338
[06:46:34.235] iteration 103 : loss: 0.330056, loss_dice: 0.348938, loss_ce: 0.311175
[06:46:35.811] iteration 104 : loss: 0.360218, loss_dice: 0.368685, loss_ce: 0.351750
[06:46:37.370] iteration 105 : loss: 0.397197, loss_dice: 0.387598, loss_ce: 0.406796
[06:46:38.942] iteration 106 : loss: 0.401656, loss_dice: 0.400151, loss_ce: 0.403160
[06:46:40.494] iteration 107 : loss: 0.384917, loss_dice: 0.375720, loss_ce: 0.394113
[06:46:42.165] iteration 108 : loss: 0.431217, loss_dice: 0.418106, loss_ce: 0.444328
[06:46:43.834] iteration 109 : loss: 0.341880, loss_dice: 0.346926, loss_ce: 0.336834
[06:46:45.393] iteration 110 : loss: 0.562884, loss_dice: 0.513236, loss_ce: 0.612533
[06:46:46.962] iteration 111 : loss: 0.375047, loss_dice: 0.363700, loss_ce: 0.386394
[06:46:48.530] iteration 112 : loss: 0.312115, loss_dice: 0.335032, loss_ce: 0.289197
[06:46:50.099] iteration 113 : loss: 0.362984, loss_dice: 0.372350, loss_ce: 0.353618
[06:46:51.657] iteration 114 : loss: 0.331815, loss_dice: 0.345587, loss_ce: 0.318043
[06:46:53.235] iteration 115 : loss: 0.359834, loss_dice: 0.363802, loss_ce: 0.355865
[06:46:54.885] iteration 116 : loss: 0.416516, loss_dice: 0.397432, loss_ce: 0.435601
[06:46:56.563] iteration 117 : loss: 0.317075, loss_dice: 0.330719, loss_ce: 0.303431
[06:46:58.129] iteration 118 : loss: 0.383026, loss_dice: 0.373541, loss_ce: 0.392512
[06:46:59.703] iteration 119 : loss: 0.335144, loss_dice: 0.334165, loss_ce: 0.336122
[06:47:01.279] iteration 120 : loss: 0.389765, loss_dice: 0.355833, loss_ce: 0.423696
[06:47:01.282] 

[06:47:02.858] iteration 121 : loss: 0.346323, loss_dice: 0.340179, loss_ce: 0.352466
[06:47:04.432] iteration 122 : loss: 0.341714, loss_dice: 0.353746, loss_ce: 0.329682
[06:47:06.001] iteration 123 : loss: 0.487321, loss_dice: 0.451607, loss_ce: 0.523036
[06:47:07.650] iteration 124 : loss: 0.340834, loss_dice: 0.332842, loss_ce: 0.348827
[06:47:09.315] iteration 125 : loss: 0.389615, loss_dice: 0.389505, loss_ce: 0.389725
[06:47:10.875] iteration 126 : loss: 0.459318, loss_dice: 0.421121, loss_ce: 0.497515
[06:47:12.474] iteration 127 : loss: 0.355929, loss_dice: 0.367170, loss_ce: 0.344687
[06:47:14.035] iteration 128 : loss: 0.412649, loss_dice: 0.413441, loss_ce: 0.411857
[06:47:15.592] iteration 129 : loss: 0.308879, loss_dice: 0.307397, loss_ce: 0.310360
[06:47:17.169] iteration 130 : loss: 0.349354, loss_dice: 0.351938, loss_ce: 0.346769
[06:47:18.732] iteration 131 : loss: 0.392346, loss_dice: 0.376109, loss_ce: 0.408584
[06:47:20.384] iteration 132 : loss: 0.330718, loss_dice: 0.342476, loss_ce: 0.318959
[06:47:22.041] iteration 133 : loss: 0.365957, loss_dice: 0.373629, loss_ce: 0.358285
[06:47:23.659] iteration 134 : loss: 0.380899, loss_dice: 0.388872, loss_ce: 0.372927
[06:47:25.214] iteration 135 : loss: 0.366681, loss_dice: 0.355070, loss_ce: 0.378292
[06:47:26.791] iteration 136 : loss: 0.382690, loss_dice: 0.372825, loss_ce: 0.392555
[06:47:28.367] iteration 137 : loss: 0.308609, loss_dice: 0.320900, loss_ce: 0.296318
[06:47:29.928] iteration 138 : loss: 0.303738, loss_dice: 0.313139, loss_ce: 0.294337
[06:47:31.483] iteration 139 : loss: 0.427995, loss_dice: 0.420681, loss_ce: 0.435309
[06:47:33.062] iteration 140 : loss: 0.391640, loss_dice: 0.384879, loss_ce: 0.398401
[06:47:34.719] iteration 141 : loss: 0.355637, loss_dice: 0.345494, loss_ce: 0.365780
[06:47:36.359] iteration 142 : loss: 0.318054, loss_dice: 0.332609, loss_ce: 0.303499
[06:47:37.934] iteration 143 : loss: 0.356972, loss_dice: 0.360878, loss_ce: 0.353066
[06:47:39.492] iteration 144 : loss: 0.341566, loss_dice: 0.334627, loss_ce: 0.348504
[06:47:41.070] iteration 145 : loss: 0.253929, loss_dice: 0.283911, loss_ce: 0.223946
[06:47:42.645] iteration 146 : loss: 0.351277, loss_dice: 0.371183, loss_ce: 0.331371
[06:47:44.204] iteration 147 : loss: 0.416926, loss_dice: 0.397084, loss_ce: 0.436769
[06:47:45.769] iteration 148 : loss: 0.301432, loss_dice: 0.286187, loss_ce: 0.316677
[06:47:47.431] iteration 149 : loss: 0.319936, loss_dice: 0.328455, loss_ce: 0.311418
[06:47:49.080] iteration 150 : loss: 0.330764, loss_dice: 0.331098, loss_ce: 0.330431
[06:47:50.633] iteration 151 : loss: 0.346135, loss_dice: 0.341172, loss_ce: 0.351098
[06:47:52.205] iteration 152 : loss: 0.316836, loss_dice: 0.312355, loss_ce: 0.321317
[06:47:53.772] iteration 153 : loss: 0.245257, loss_dice: 0.263362, loss_ce: 0.227153
[06:47:55.338] iteration 154 : loss: 0.258853, loss_dice: 0.266672, loss_ce: 0.251034
[06:47:56.889] iteration 155 : loss: 0.246864, loss_dice: 0.262463, loss_ce: 0.231266
[06:47:58.453] iteration 156 : loss: 0.275841, loss_dice: 0.283954, loss_ce: 0.267728
[06:48:00.102] iteration 157 : loss: 0.341883, loss_dice: 0.324661, loss_ce: 0.359105
[06:48:01.762] iteration 158 : loss: 0.326772, loss_dice: 0.332648, loss_ce: 0.320896
[06:48:03.320] iteration 159 : loss: 0.300512, loss_dice: 0.317835, loss_ce: 0.283188
[06:48:04.878] iteration 160 : loss: 0.247556, loss_dice: 0.250614, loss_ce: 0.244498
[06:48:04.879] 

[06:48:06.434] iteration 161 : loss: 0.263053, loss_dice: 0.260065, loss_ce: 0.266042
[06:48:08.012] iteration 162 : loss: 0.311962, loss_dice: 0.303132, loss_ce: 0.320792
[06:48:09.588] iteration 163 : loss: 0.306451, loss_dice: 0.299145, loss_ce: 0.313756
[06:48:11.146] iteration 164 : loss: 0.299864, loss_dice: 0.306806, loss_ce: 0.292922
[06:48:12.770] iteration 165 : loss: 0.335537, loss_dice: 0.333436, loss_ce: 0.337638
[06:48:14.478] iteration 166 : loss: 0.361997, loss_dice: 0.358700, loss_ce: 0.365294
[06:48:16.057] iteration 167 : loss: 0.280011, loss_dice: 0.282915, loss_ce: 0.277108
[06:48:17.633] iteration 168 : loss: 0.287389, loss_dice: 0.285628, loss_ce: 0.289150
[06:48:19.200] iteration 169 : loss: 0.349689, loss_dice: 0.369816, loss_ce: 0.329562
[06:48:20.770] iteration 170 : loss: 0.376769, loss_dice: 0.368264, loss_ce: 0.385274
[06:48:22.353] iteration 171 : loss: 0.263087, loss_dice: 0.273619, loss_ce: 0.252554
[06:48:23.925] iteration 172 : loss: 0.300292, loss_dice: 0.295814, loss_ce: 0.304770
[06:48:25.483] iteration 173 : loss: 0.389954, loss_dice: 0.401359, loss_ce: 0.378548
[06:48:27.148] iteration 174 : loss: 0.240088, loss_dice: 0.258676, loss_ce: 0.221499
[06:48:28.799] iteration 175 : loss: 0.437514, loss_dice: 0.383385, loss_ce: 0.491643
[06:48:30.364] iteration 176 : loss: 0.239175, loss_dice: 0.259621, loss_ce: 0.218728
[06:48:31.932] iteration 177 : loss: 0.361429, loss_dice: 0.350995, loss_ce: 0.371863
[06:48:33.500] iteration 178 : loss: 0.287108, loss_dice: 0.326704, loss_ce: 0.247512
[06:48:35.105] iteration 179 : loss: 0.308787, loss_dice: 0.333138, loss_ce: 0.284437
[06:48:36.683] iteration 180 : loss: 0.381459, loss_dice: 0.380217, loss_ce: 0.382702
[06:48:38.241] iteration 181 : loss: 0.229271, loss_dice: 0.246887, loss_ce: 0.211656
[06:48:39.917] iteration 182 : loss: 0.336656, loss_dice: 0.356232, loss_ce: 0.317081
[06:48:41.586] iteration 183 : loss: 0.208159, loss_dice: 0.233109, loss_ce: 0.183208
[06:48:43.150] iteration 184 : loss: 0.255920, loss_dice: 0.264667, loss_ce: 0.247172
[06:48:44.728] iteration 185 : loss: 0.266026, loss_dice: 0.267747, loss_ce: 0.264304
[06:48:46.385] iteration 186 : loss: 0.266687, loss_dice: 0.285324, loss_ce: 0.248051
[06:48:48.051] iteration 187 : loss: 0.395229, loss_dice: 0.403027, loss_ce: 0.387431
[06:48:49.619] iteration 188 : loss: 0.306762, loss_dice: 0.314700, loss_ce: 0.298824
[06:48:51.199] iteration 189 : loss: 0.371039, loss_dice: 0.364497, loss_ce: 0.377581
[06:48:52.858] iteration 190 : loss: 0.291449, loss_dice: 0.303515, loss_ce: 0.279384
[06:48:54.533] iteration 191 : loss: 0.370836, loss_dice: 0.371888, loss_ce: 0.369784
[06:48:56.093] iteration 192 : loss: 0.329227, loss_dice: 0.340333, loss_ce: 0.318121
[06:48:57.671] iteration 193 : loss: 0.323742, loss_dice: 0.343340, loss_ce: 0.304144
[06:48:59.236] iteration 194 : loss: 0.257457, loss_dice: 0.276261, loss_ce: 0.238652
[06:49:00.808] iteration 195 : loss: 0.301849, loss_dice: 0.332603, loss_ce: 0.271095
[06:49:02.378] iteration 196 : loss: 0.304273, loss_dice: 0.285365, loss_ce: 0.323180
[06:49:03.946] iteration 197 : loss: 0.369032, loss_dice: 0.367973, loss_ce: 0.370091
[06:49:05.616] iteration 198 : loss: 0.304952, loss_dice: 0.334151, loss_ce: 0.275754
[06:49:07.283] iteration 199 : loss: 0.226043, loss_dice: 0.239737, loss_ce: 0.212349
[06:49:08.860] iteration 200 : loss: 0.306053, loss_dice: 0.320133, loss_ce: 0.291972
[06:56:53.366] Namespace(root_path='/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI', exp='SDCL', model='VNet', pre_max_iteration=2000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.001, deterministic=1, labelnum=8, gpu='0', seed=1345, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[06:56:53.931] train_lab set: total 8 samples
[06:56:53.931] total ['/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/06SR5RBREL16DQ6M8LWS/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/0RZDK210BSMWAA6467LU/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1D7CUD1955YZPGK8XHJX/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1GU15S0GJ6PFNARO469W/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1MHBF3G6DCPWHSKG7XCP/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/23X6SY44VT9KFHR7S7OC/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/2XL5HSFSE93RMOJDRGR4/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/38CWS74285MFGZZXR09Z/mri_norm2.h5'] samples
[06:56:53.935] train_lab set: total 8 samples
[06:56:53.936] total ['/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/06SR5RBREL16DQ6M8LWS/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/0RZDK210BSMWAA6467LU/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1D7CUD1955YZPGK8XHJX/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1GU15S0GJ6PFNARO469W/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1MHBF3G6DCPWHSKG7XCP/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/23X6SY44VT9KFHR7S7OC/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/2XL5HSFSE93RMOJDRGR4/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/38CWS74285MFGZZXR09Z/mri_norm2.h5'] samples
[06:56:53.938] 40 iterations per epoch
[06:56:53.939] 

[06:56:55.854] iteration 1 : loss: 0.809023, loss_dice: 0.619381, loss_ce: 0.998665
[06:56:57.383] iteration 2 : loss: 0.776670, loss_dice: 0.597845, loss_ce: 0.955496
[06:56:58.886] iteration 3 : loss: 0.785931, loss_dice: 0.565157, loss_ce: 1.006706
[06:57:00.379] iteration 4 : loss: 0.665670, loss_dice: 0.514858, loss_ce: 0.816481
[06:57:01.874] iteration 5 : loss: 0.720878, loss_dice: 0.552512, loss_ce: 0.889245
[06:57:03.358] iteration 6 : loss: 0.626831, loss_dice: 0.535774, loss_ce: 0.717888
[06:57:04.936] iteration 7 : loss: 0.655470, loss_dice: 0.538879, loss_ce: 0.772060
[06:57:06.542] iteration 8 : loss: 0.700820, loss_dice: 0.576487, loss_ce: 0.825153
[06:57:08.051] iteration 9 : loss: 0.647019, loss_dice: 0.522661, loss_ce: 0.771377
[06:57:09.546] iteration 10 : loss: 0.615736, loss_dice: 0.522275, loss_ce: 0.709197
[06:57:11.038] iteration 11 : loss: 0.630263, loss_dice: 0.535606, loss_ce: 0.724919
[06:57:12.667] iteration 12 : loss: 0.670949, loss_dice: 0.563697, loss_ce: 0.778202
[06:57:14.375] iteration 13 : loss: 0.626526, loss_dice: 0.531732, loss_ce: 0.721320
[06:57:16.147] iteration 14 : loss: 0.584581, loss_dice: 0.524292, loss_ce: 0.644871
[06:57:17.740] iteration 15 : loss: 0.638324, loss_dice: 0.555519, loss_ce: 0.721130
[06:57:19.343] iteration 16 : loss: 0.634524, loss_dice: 0.556194, loss_ce: 0.712854
[06:57:20.858] iteration 17 : loss: 0.584055, loss_dice: 0.532808, loss_ce: 0.635303
[06:57:22.397] iteration 18 : loss: 0.666695, loss_dice: 0.567086, loss_ce: 0.766303
[06:57:23.910] iteration 19 : loss: 0.521363, loss_dice: 0.488654, loss_ce: 0.554072
[06:57:25.433] iteration 20 : loss: 0.535987, loss_dice: 0.489677, loss_ce: 0.582298
[06:57:26.953] iteration 21 : loss: 0.536015, loss_dice: 0.497616, loss_ce: 0.574414
[06:57:28.470] iteration 22 : loss: 0.583518, loss_dice: 0.525945, loss_ce: 0.641091
[06:57:29.988] iteration 23 : loss: 0.545214, loss_dice: 0.510769, loss_ce: 0.579659
[06:57:31.610] iteration 24 : loss: 0.544053, loss_dice: 0.498306, loss_ce: 0.589799
[06:57:33.224] iteration 25 : loss: 0.570813, loss_dice: 0.533525, loss_ce: 0.608101
[06:57:34.741] iteration 26 : loss: 0.537754, loss_dice: 0.500269, loss_ce: 0.575240
[06:57:36.277] iteration 27 : loss: 0.498025, loss_dice: 0.464363, loss_ce: 0.531688
[06:57:37.797] iteration 28 : loss: 0.592705, loss_dice: 0.538146, loss_ce: 0.647265
[06:57:39.351] iteration 29 : loss: 0.587886, loss_dice: 0.522553, loss_ce: 0.653219
[06:57:40.970] iteration 30 : loss: 0.520495, loss_dice: 0.500252, loss_ce: 0.540738
[06:57:42.588] iteration 31 : loss: 0.603025, loss_dice: 0.538469, loss_ce: 0.667581
[06:57:44.206] iteration 32 : loss: 0.530934, loss_dice: 0.509333, loss_ce: 0.552535
[06:57:45.842] iteration 33 : loss: 0.511278, loss_dice: 0.490907, loss_ce: 0.531649
[06:57:47.382] iteration 34 : loss: 0.507734, loss_dice: 0.491652, loss_ce: 0.523815
[06:57:48.920] iteration 35 : loss: 0.536078, loss_dice: 0.499046, loss_ce: 0.573110
[06:57:50.458] iteration 36 : loss: 0.516046, loss_dice: 0.470749, loss_ce: 0.561342
[06:57:51.997] iteration 37 : loss: 0.451570, loss_dice: 0.438101, loss_ce: 0.465038
[06:57:53.562] iteration 38 : loss: 0.538252, loss_dice: 0.488556, loss_ce: 0.587948
[06:57:55.094] iteration 39 : loss: 0.444494, loss_dice: 0.438989, loss_ce: 0.449998
[06:57:56.745] iteration 40 : loss: 0.502738, loss_dice: 0.463060, loss_ce: 0.542416
[06:57:56.752] 

[06:57:58.379] iteration 41 : loss: 0.369668, loss_dice: 0.360502, loss_ce: 0.378835
[06:57:59.939] iteration 42 : loss: 0.525261, loss_dice: 0.491476, loss_ce: 0.559047
[06:58:01.509] iteration 43 : loss: 0.460210, loss_dice: 0.443789, loss_ce: 0.476630
[06:58:03.053] iteration 44 : loss: 0.481459, loss_dice: 0.449945, loss_ce: 0.512973
[06:58:04.615] iteration 45 : loss: 0.579384, loss_dice: 0.519912, loss_ce: 0.638856
[06:58:06.173] iteration 46 : loss: 0.500483, loss_dice: 0.453964, loss_ce: 0.547001
[06:58:07.732] iteration 47 : loss: 0.508519, loss_dice: 0.486690, loss_ce: 0.530348
[06:58:09.423] iteration 48 : loss: 0.451762, loss_dice: 0.425508, loss_ce: 0.478016
[06:58:11.071] iteration 49 : loss: 0.501925, loss_dice: 0.461574, loss_ce: 0.542276
[06:58:12.641] iteration 50 : loss: 0.504351, loss_dice: 0.466321, loss_ce: 0.542380
[06:58:14.231] iteration 51 : loss: 0.535703, loss_dice: 0.487452, loss_ce: 0.583955
[06:58:15.790] iteration 52 : loss: 0.467291, loss_dice: 0.439529, loss_ce: 0.495053
[06:58:17.369] iteration 53 : loss: 0.390303, loss_dice: 0.384152, loss_ce: 0.396454
[06:58:18.928] iteration 54 : loss: 0.474041, loss_dice: 0.459052, loss_ce: 0.489031
[06:58:20.503] iteration 55 : loss: 0.433785, loss_dice: 0.427717, loss_ce: 0.439852
[06:58:22.082] iteration 56 : loss: 0.500657, loss_dice: 0.470226, loss_ce: 0.531089
[06:58:23.760] iteration 57 : loss: 0.540288, loss_dice: 0.488922, loss_ce: 0.591655
[06:58:25.438] iteration 58 : loss: 0.383081, loss_dice: 0.373316, loss_ce: 0.392845
[06:58:27.015] iteration 59 : loss: 0.573497, loss_dice: 0.511524, loss_ce: 0.635469
[06:58:28.611] iteration 60 : loss: 0.507754, loss_dice: 0.467392, loss_ce: 0.548115
[06:58:30.190] iteration 61 : loss: 0.430911, loss_dice: 0.407286, loss_ce: 0.454535
[06:58:31.779] iteration 62 : loss: 0.421285, loss_dice: 0.412535, loss_ce: 0.430035
[06:58:33.359] iteration 63 : loss: 0.396000, loss_dice: 0.402392, loss_ce: 0.389609
[06:58:34.941] iteration 64 : loss: 0.408336, loss_dice: 0.392893, loss_ce: 0.423779
[06:58:36.615] iteration 65 : loss: 0.447444, loss_dice: 0.439368, loss_ce: 0.455520
[06:58:38.305] iteration 66 : loss: 0.433443, loss_dice: 0.425367, loss_ce: 0.441518
[06:58:39.903] iteration 67 : loss: 0.521803, loss_dice: 0.478461, loss_ce: 0.565144
[06:58:41.527] iteration 68 : loss: 0.521286, loss_dice: 0.468890, loss_ce: 0.573682
[06:58:43.144] iteration 69 : loss: 0.517220, loss_dice: 0.481008, loss_ce: 0.553432
[06:58:44.734] iteration 70 : loss: 0.498435, loss_dice: 0.491558, loss_ce: 0.505311
[06:58:46.326] iteration 71 : loss: 0.505150, loss_dice: 0.440431, loss_ce: 0.569870
[06:58:47.919] iteration 72 : loss: 0.403098, loss_dice: 0.389200, loss_ce: 0.416996
[06:58:49.599] iteration 73 : loss: 0.325600, loss_dice: 0.322374, loss_ce: 0.328825
[06:58:51.289] iteration 74 : loss: 0.371274, loss_dice: 0.375825, loss_ce: 0.366723
[06:58:52.871] iteration 75 : loss: 0.398667, loss_dice: 0.400769, loss_ce: 0.396564
[06:58:54.449] iteration 76 : loss: 0.395835, loss_dice: 0.385384, loss_ce: 0.406286
[06:58:56.024] iteration 77 : loss: 0.383837, loss_dice: 0.387481, loss_ce: 0.380193
[06:58:57.605] iteration 78 : loss: 0.457006, loss_dice: 0.447059, loss_ce: 0.466953
[06:58:59.187] iteration 79 : loss: 0.412130, loss_dice: 0.411393, loss_ce: 0.412866
[06:59:00.764] iteration 80 : loss: 0.533593, loss_dice: 0.471139, loss_ce: 0.596047
[06:59:00.772] 

[06:59:02.422] iteration 81 : loss: 0.394834, loss_dice: 0.391109, loss_ce: 0.398560
[06:59:04.099] iteration 82 : loss: 0.445826, loss_dice: 0.427247, loss_ce: 0.464405
[06:59:05.683] iteration 83 : loss: 0.575721, loss_dice: 0.495181, loss_ce: 0.656261
[06:59:07.247] iteration 84 : loss: 0.385936, loss_dice: 0.378498, loss_ce: 0.393374
[06:59:08.835] iteration 85 : loss: 0.349445, loss_dice: 0.337033, loss_ce: 0.361857
[06:59:10.401] iteration 86 : loss: 0.413406, loss_dice: 0.424441, loss_ce: 0.402371
[06:59:11.970] iteration 87 : loss: 0.410751, loss_dice: 0.411522, loss_ce: 0.409980
[06:59:13.549] iteration 88 : loss: 0.429369, loss_dice: 0.425007, loss_ce: 0.433731
[06:59:15.223] iteration 89 : loss: 0.412751, loss_dice: 0.384578, loss_ce: 0.440923
[06:59:16.870] iteration 90 : loss: 0.406628, loss_dice: 0.388043, loss_ce: 0.425212
[06:59:18.479] iteration 91 : loss: 0.428531, loss_dice: 0.415259, loss_ce: 0.441803
[06:59:20.036] iteration 92 : loss: 0.381459, loss_dice: 0.386831, loss_ce: 0.376087
[06:59:21.614] iteration 93 : loss: 0.504758, loss_dice: 0.467501, loss_ce: 0.542015
[06:59:23.179] iteration 94 : loss: 0.349972, loss_dice: 0.336536, loss_ce: 0.363408
[06:59:24.748] iteration 95 : loss: 0.427669, loss_dice: 0.412029, loss_ce: 0.443310
[06:59:26.314] iteration 96 : loss: 0.457980, loss_dice: 0.426202, loss_ce: 0.489758
[06:59:27.891] iteration 97 : loss: 0.292081, loss_dice: 0.304425, loss_ce: 0.279737
[06:59:29.543] iteration 98 : loss: 0.379233, loss_dice: 0.391223, loss_ce: 0.367242
[06:59:31.110] iteration 99 : loss: 0.436070, loss_dice: 0.405367, loss_ce: 0.466772
[06:59:32.716] iteration 100 : loss: 0.378527, loss_dice: 0.368376, loss_ce: 0.388678
[06:59:34.286] iteration 101 : loss: 0.367766, loss_dice: 0.352724, loss_ce: 0.382809
[06:59:35.852] iteration 102 : loss: 0.429485, loss_dice: 0.410633, loss_ce: 0.448338
[06:59:37.427] iteration 103 : loss: 0.330056, loss_dice: 0.348938, loss_ce: 0.311175
[06:59:39.004] iteration 104 : loss: 0.360218, loss_dice: 0.368685, loss_ce: 0.351750
[06:59:40.579] iteration 105 : loss: 0.397197, loss_dice: 0.387598, loss_ce: 0.406796
[06:59:42.238] iteration 106 : loss: 0.401656, loss_dice: 0.400151, loss_ce: 0.403160
[06:59:43.896] iteration 107 : loss: 0.384917, loss_dice: 0.375720, loss_ce: 0.394113
[06:59:45.471] iteration 108 : loss: 0.431217, loss_dice: 0.418106, loss_ce: 0.444328
[06:59:47.048] iteration 109 : loss: 0.341880, loss_dice: 0.346926, loss_ce: 0.336834
[06:59:48.646] iteration 110 : loss: 0.562885, loss_dice: 0.513236, loss_ce: 0.612533
[06:59:50.224] iteration 111 : loss: 0.375047, loss_dice: 0.363700, loss_ce: 0.386394
[06:59:51.801] iteration 112 : loss: 0.312115, loss_dice: 0.335032, loss_ce: 0.289197
[06:59:53.360] iteration 113 : loss: 0.362984, loss_dice: 0.372350, loss_ce: 0.353618
[06:59:55.037] iteration 114 : loss: 0.331815, loss_dice: 0.345587, loss_ce: 0.318043
[06:59:56.712] iteration 115 : loss: 0.359834, loss_dice: 0.363802, loss_ce: 0.355865
[06:59:58.285] iteration 116 : loss: 0.416516, loss_dice: 0.397432, loss_ce: 0.435601
[06:59:59.868] iteration 117 : loss: 0.317075, loss_dice: 0.330719, loss_ce: 0.303431
[07:00:01.445] iteration 118 : loss: 0.383026, loss_dice: 0.373541, loss_ce: 0.392512
[07:00:03.032] iteration 119 : loss: 0.335144, loss_dice: 0.334165, loss_ce: 0.336122
[07:00:04.601] iteration 120 : loss: 0.389765, loss_dice: 0.355833, loss_ce: 0.423696
[07:00:04.605] 

[07:00:06.167] iteration 121 : loss: 0.346323, loss_dice: 0.340179, loss_ce: 0.352466
[07:00:07.885] iteration 122 : loss: 0.341714, loss_dice: 0.353746, loss_ce: 0.329682
[07:00:09.556] iteration 123 : loss: 0.487321, loss_dice: 0.451607, loss_ce: 0.523036
[07:00:11.131] iteration 124 : loss: 0.340834, loss_dice: 0.332842, loss_ce: 0.348827
[07:00:12.697] iteration 125 : loss: 0.389615, loss_dice: 0.389505, loss_ce: 0.389725
[07:00:14.274] iteration 126 : loss: 0.459318, loss_dice: 0.421121, loss_ce: 0.497515
[07:00:15.846] iteration 127 : loss: 0.355929, loss_dice: 0.367170, loss_ce: 0.344687
[07:00:17.414] iteration 128 : loss: 0.412649, loss_dice: 0.413441, loss_ce: 0.411857
[07:00:19.018] iteration 129 : loss: 0.308879, loss_dice: 0.307397, loss_ce: 0.310360
[07:00:20.687] iteration 130 : loss: 0.349354, loss_dice: 0.351938, loss_ce: 0.346769
[07:00:22.359] iteration 131 : loss: 0.392346, loss_dice: 0.376109, loss_ce: 0.408584
[07:00:23.942] iteration 132 : loss: 0.330718, loss_dice: 0.342476, loss_ce: 0.318959
[07:00:25.501] iteration 133 : loss: 0.365957, loss_dice: 0.373629, loss_ce: 0.358285
[07:00:27.076] iteration 134 : loss: 0.380899, loss_dice: 0.388872, loss_ce: 0.372927
[07:00:28.653] iteration 135 : loss: 0.366681, loss_dice: 0.355070, loss_ce: 0.378292
[07:00:30.232] iteration 136 : loss: 0.382690, loss_dice: 0.372825, loss_ce: 0.392555
[07:00:31.827] iteration 137 : loss: 0.308609, loss_dice: 0.320900, loss_ce: 0.296318
[07:00:33.494] iteration 138 : loss: 0.303738, loss_dice: 0.313139, loss_ce: 0.294337
[07:00:35.154] iteration 139 : loss: 0.427995, loss_dice: 0.420681, loss_ce: 0.435309
[07:00:36.739] iteration 140 : loss: 0.391640, loss_dice: 0.384879, loss_ce: 0.398401
[07:00:38.303] iteration 141 : loss: 0.355637, loss_dice: 0.345494, loss_ce: 0.365780
[07:00:39.892] iteration 142 : loss: 0.318054, loss_dice: 0.332609, loss_ce: 0.303499
[07:00:41.489] iteration 143 : loss: 0.356972, loss_dice: 0.360878, loss_ce: 0.353066
[07:00:43.067] iteration 144 : loss: 0.341566, loss_dice: 0.334627, loss_ce: 0.348504
[07:00:44.640] iteration 145 : loss: 0.253929, loss_dice: 0.283911, loss_ce: 0.223946
[07:00:46.216] iteration 146 : loss: 0.351277, loss_dice: 0.371183, loss_ce: 0.331371
[07:00:47.877] iteration 147 : loss: 0.416926, loss_dice: 0.397084, loss_ce: 0.436769
[07:00:49.496] iteration 148 : loss: 0.301432, loss_dice: 0.286187, loss_ce: 0.316677
[07:00:51.093] iteration 149 : loss: 0.319936, loss_dice: 0.328455, loss_ce: 0.311418
[07:00:52.663] iteration 150 : loss: 0.330764, loss_dice: 0.331098, loss_ce: 0.330431
[07:00:54.248] iteration 151 : loss: 0.346135, loss_dice: 0.341172, loss_ce: 0.351098
[07:00:55.856] iteration 152 : loss: 0.316836, loss_dice: 0.312355, loss_ce: 0.321317
[07:00:57.444] iteration 153 : loss: 0.245257, loss_dice: 0.263362, loss_ce: 0.227153
[07:00:59.021] iteration 154 : loss: 0.258853, loss_dice: 0.266672, loss_ce: 0.251034
[07:01:00.690] iteration 155 : loss: 0.246864, loss_dice: 0.262463, loss_ce: 0.231266
[07:01:02.362] iteration 156 : loss: 0.275841, loss_dice: 0.283954, loss_ce: 0.267728
[07:01:03.934] iteration 157 : loss: 0.341883, loss_dice: 0.324661, loss_ce: 0.359105
[07:01:05.512] iteration 158 : loss: 0.326772, loss_dice: 0.332648, loss_ce: 0.320896
[07:01:07.092] iteration 159 : loss: 0.300512, loss_dice: 0.317835, loss_ce: 0.283188
[07:01:08.688] iteration 160 : loss: 0.247556, loss_dice: 0.250614, loss_ce: 0.244498
[07:01:08.689] 

[07:01:10.263] iteration 161 : loss: 0.263053, loss_dice: 0.260065, loss_ce: 0.266042
[07:01:11.862] iteration 162 : loss: 0.311962, loss_dice: 0.303132, loss_ce: 0.320792
[07:01:13.539] iteration 163 : loss: 0.306451, loss_dice: 0.299145, loss_ce: 0.313756
[07:01:15.219] iteration 164 : loss: 0.299864, loss_dice: 0.306806, loss_ce: 0.292922
[07:01:16.818] iteration 165 : loss: 0.335537, loss_dice: 0.333436, loss_ce: 0.337638
[07:01:18.405] iteration 166 : loss: 0.361997, loss_dice: 0.358700, loss_ce: 0.365294
[07:01:19.995] iteration 167 : loss: 0.280011, loss_dice: 0.282915, loss_ce: 0.277108
[07:01:21.573] iteration 168 : loss: 0.287389, loss_dice: 0.285628, loss_ce: 0.289150
[07:01:23.171] iteration 169 : loss: 0.349689, loss_dice: 0.369816, loss_ce: 0.329562
[07:01:24.749] iteration 170 : loss: 0.376769, loss_dice: 0.368264, loss_ce: 0.385274
[07:01:26.407] iteration 171 : loss: 0.263087, loss_dice: 0.273619, loss_ce: 0.252554
[07:01:28.098] iteration 172 : loss: 0.300292, loss_dice: 0.295814, loss_ce: 0.304770
[07:01:29.683] iteration 173 : loss: 0.389954, loss_dice: 0.401359, loss_ce: 0.378548
[07:01:31.279] iteration 174 : loss: 0.240088, loss_dice: 0.258676, loss_ce: 0.221499
[07:01:32.846] iteration 175 : loss: 0.437514, loss_dice: 0.383385, loss_ce: 0.491643
[07:01:34.422] iteration 176 : loss: 0.239175, loss_dice: 0.259621, loss_ce: 0.218728
[07:01:35.992] iteration 177 : loss: 0.361429, loss_dice: 0.350995, loss_ce: 0.371863
[07:01:37.572] iteration 178 : loss: 0.287108, loss_dice: 0.326704, loss_ce: 0.247512
[07:01:39.228] iteration 179 : loss: 0.308787, loss_dice: 0.333138, loss_ce: 0.284437
[07:01:40.885] iteration 180 : loss: 0.381459, loss_dice: 0.380217, loss_ce: 0.382702
[07:01:42.463] iteration 181 : loss: 0.229271, loss_dice: 0.246887, loss_ce: 0.211656
[07:01:44.040] iteration 182 : loss: 0.336656, loss_dice: 0.356232, loss_ce: 0.317081
[07:01:45.617] iteration 183 : loss: 0.208159, loss_dice: 0.233109, loss_ce: 0.183208
[07:01:47.193] iteration 184 : loss: 0.255920, loss_dice: 0.264667, loss_ce: 0.247172
[07:01:48.773] iteration 185 : loss: 0.266026, loss_dice: 0.267747, loss_ce: 0.264304
[07:01:50.340] iteration 186 : loss: 0.266687, loss_dice: 0.285324, loss_ce: 0.248051
[07:01:52.022] iteration 187 : loss: 0.395229, loss_dice: 0.403027, loss_ce: 0.387431
[07:01:53.683] iteration 188 : loss: 0.306762, loss_dice: 0.314700, loss_ce: 0.298824
[07:01:55.278] iteration 189 : loss: 0.371039, loss_dice: 0.364497, loss_ce: 0.377581
[07:01:56.851] iteration 190 : loss: 0.291449, loss_dice: 0.303515, loss_ce: 0.279384
[07:01:58.411] iteration 191 : loss: 0.370836, loss_dice: 0.371888, loss_ce: 0.369784
[07:02:00.009] iteration 192 : loss: 0.329227, loss_dice: 0.340333, loss_ce: 0.318121
[07:02:01.596] iteration 193 : loss: 0.323742, loss_dice: 0.343340, loss_ce: 0.304144
[07:02:03.161] iteration 194 : loss: 0.257457, loss_dice: 0.276261, loss_ce: 0.238652
[07:02:04.738] iteration 195 : loss: 0.301849, loss_dice: 0.332603, loss_ce: 0.271095
[07:02:06.400] iteration 196 : loss: 0.304272, loss_dice: 0.285365, loss_ce: 0.323180
[07:02:08.024] iteration 197 : loss: 0.369032, loss_dice: 0.367973, loss_ce: 0.370091
[07:02:09.678] iteration 198 : loss: 0.304952, loss_dice: 0.334151, loss_ce: 0.275754
[07:02:11.329] iteration 199 : loss: 0.226043, loss_dice: 0.239737, loss_ce: 0.212349
[07:02:12.903] iteration 200 : loss: 0.306053, loss_dice: 0.320133, loss_ce: 0.291972
